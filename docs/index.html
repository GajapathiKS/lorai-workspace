<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LorAI — All of AI. One Command.</title>
  <meta name="description" content="Local, free, OpenAI-compatible AI platform. LLMs, image gen, voice, video, RAG, agents, vision — all from one pip install. Port 1842.">
  <meta name="keywords" content="LorAI, AI platform, local AI, OpenAI compatible, LLM, image generation, voice, RAG, agents, Docker, Ollama, self-hosted AI">
  <meta name="author" content="LorAI Team">
  <meta name="robots" content="index, follow">
  <link rel="canonical" href="https://gajapathiks.github.io/lorai-workspace/">

  <!-- Open Graph / Facebook / LinkedIn -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://gajapathiks.github.io/lorai-workspace/">
  <meta property="og:title" content="LorAI — All of AI. One Command.">
  <meta property="og:description" content="Local, free, OpenAI-compatible AI platform with 50+ tools — LLMs, image gen, voice, video, RAG, agents — running on port 1842.">
  <meta property="og:image" content="https://gajapathiks.github.io/lorai-workspace/og-image.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="LorAI — All of AI. One Command. pip install lorai-workspace. Port 1842.">
  <meta property="og:site_name" content="LorAI">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="LorAI — All of AI. One Command.">
  <meta name="twitter:description" content="Local, free, OpenAI-compatible AI platform with 50+ tools — LLMs, image gen, voice, video, RAG, agents — running on port 1842.">
  <meta name="twitter:image" content="https://gajapathiks.github.io/lorai-workspace/og-image.png">
  <meta name="twitter:image:alt" content="LorAI — All of AI. One Command. pip install lorai-workspace. Port 1842.">

  <!-- Theme -->
  <meta name="theme-color" content="#0d1117">
  <meta name="color-scheme" content="dark">

  <link rel="stylesheet" href="style.css">
</head>
<body>

<nav>
  <div class="nav-inner">
    <a href="index.html" class="logo">Lor<span>AI</span></a>
    <div class="nav-links">
      <a href="index.html" class="active">Home</a>
      <a href="guide.html">Guide</a>
      <a href="sdk.html">SDK</a>
      <a href="api.html">API</a>
      <a href="integrations.html">Integrations</a>
      <a href="https://github.com/GajapathiKS/lorai-workspace">GitHub</a>
    </div>
  </div>
</nav>

<main>

<!-- Hero -->
<section class="hero">
<pre class="banner">  ██╗      ██████╗  ██████╗  █████╗ ██╗
  ██║     ██╔═══██╗██╔══██╗██╔══██╗██║
  ██║     ██║   ██║██████╔╝███████║██║
  ██║     ██║   ██║██╔══██╗██╔══██║██║
  ███████╗╚██████╔╝██║  ██║██║  ██║██║
  ╚══════╝ ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝</pre>
  <p><span class="badge badge-ready">v0.1.0-beta</span></p>
  <h1>All of AI. One Command.</h1>
  <p class="tagline">Local, free, OpenAI-compatible AI platform. LLMs, image gen, voice, video, RAG, agents &mdash; port <span class="port-badge">1842</span></p>

  <div class="install-box">
    <span class="prompt">$</span> pip install lorai-workspace
  </div>

  <div class="cta-links">
    <a href="guide.html" class="primary">Get Started</a>
    <a href="api.html" class="secondary">API Reference</a>
    <a href="integrations.html" class="secondary">Integrations</a>
    <a href="https://github.com/GajapathiKS/lorai-workspace" class="secondary">GitHub</a>
  </div>
</section>

<!-- What is LorAI -->
<h2>What is LorAI?</h2>
<p>
  <strong>LorAI</strong> ("Lore" + "AI" + "LoRA") is a self-hosted AI platform that runs inside a Docker container on your machine.
  One <code>pip install lorai-workspace</code> gives you an OpenAI-compatible API with 50+ tools &mdash; LLMs, image generation, video,
  voice, code execution, agents, RAG, and vision &mdash; all on <code>localhost:1842</code>.
</p>
<p>
  It extends the official <a href="https://github.com/openai/openai-python">OpenAI Python SDK</a>, so any existing OpenAI code works
  unchanged &mdash; just swap <code>OpenAI()</code> for <code>LorAI()</code>.
</p>

<pre><code><span class="kw">from</span> lorai <span class="kw">import</span> LorAI

ai = LorAI()                          <span class="cmt"># auto-pulls Docker image, starts container</span>
<span class="fn">print</span>(ai.chat(<span class="str">"Hello!"</span>))              <span class="cmt"># local LLM response</span>

<span class="cmt"># Works exactly like OpenAI:</span>
response = ai.chat.completions.create(
    model=<span class="str">"phi3:mini"</span>,
    messages=[{<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"Explain quantum computing"</span>}]
)
</code></pre>

<!-- Works with Any Language -->
<h2>Works with Any Language</h2>
<p>
  LorAI's API is <strong>OpenAI-compatible</strong>. Any programming language with an OpenAI SDK can use LorAI
  out of the box &mdash; just set <code>base_url</code> to <code>http://localhost:1842/v1</code>.
</p>

<div class="lang-grid">
  <div class="lang-card">
    <h4>Python</h4>
    <pre><code><span class="kw">from</span> openai <span class="kw">import</span> OpenAI
client = OpenAI(
  base_url=<span class="str">"http://localhost:1842/v1"</span>,
  api_key=<span class="str">"not-needed"</span>
)</code></pre>
  </div>
  <div class="lang-card">
    <h4>Node.js</h4>
    <pre><code><span class="kw">import</span> OpenAI <span class="kw">from</span> <span class="str">"openai"</span>;
<span class="kw">const</span> client = <span class="kw">new</span> <span class="fn">OpenAI</span>({
  baseURL: <span class="str">"http://localhost:1842/v1"</span>,
  apiKey: <span class="str">"not-needed"</span>,
});</code></pre>
  </div>
  <div class="lang-card">
    <h4>Go</h4>
    <pre><code>config := openai.<span class="fn">DefaultConfig</span>(<span class="str">"not-needed"</span>)
config.BaseURL = <span class="str">"http://localhost:1842/v1"</span>
client := openai.<span class="fn">NewClientWithConfig</span>(config)</code></pre>
  </div>
  <div class="lang-card">
    <h4>cURL</h4>
    <pre><code>curl http://localhost:1842/v1/chat/completions \
  -H <span class="str">"Content-Type: application/json"</span> \
  -d <span class="str">'{"model":"phi3:mini","messages":[
    {"role":"user","content":"Hello!"}]}'</span></code></pre>
  </div>
</div>

<p style="text-align: center; margin-top: 16px;">
  <a href="integrations.html">View all 10 language integrations &rarr;</a>
  &nbsp;&nbsp;|&nbsp;&nbsp;
  Python, Node.js, Go, Rust, Java, Kotlin, C#, Ruby, PHP, cURL
</p>

<!-- Features -->
<h2>Features</h2>

<div class="features">
  <div class="feature-card">
    <span class="badge badge-ready">Ready</span>
    <h4>Chat &amp; LLMs</h4>
    <p>OpenAI-compatible chat completions, text completions, and embeddings via Ollama. Streaming supported.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-ready">Ready</span>
    <h4>Model Hub</h4>
    <p>Pull, list, remove, and benchmark Ollama models. Default: phi3:mini (runs on CPU).</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-gpu">GPU</span>
    <h4>Image Generation</h4>
    <p>SDXL Turbo text-to-image and image editing. Requires GPU with 8GB+ VRAM.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-gpu">GPU</span>
    <h4>Video Generation</h4>
    <p>CogVideoX text-to-video. Requires GPU with 12GB+ VRAM.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-ready">Ready</span>
    <h4>Voice (STT + TTS)</h4>
    <p>Whisper.cpp speech-to-text and Piper text-to-speech with 6 voice options.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-ready">Ready</span>
    <h4>Knowledge / RAG</h4>
    <p>ChromaDB-backed document ingestion, semantic search, and RAG question answering.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-ready">Ready</span>
    <h4>Agents</h4>
    <p>ReAct agent loop with 5 built-in tools: search, read, write, run, knowledge.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-ready">Ready</span>
    <h4>Code Execution</h4>
    <p>Sandboxed Python, Bash, and JavaScript execution inside the container.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-ready">Ready</span>
    <h4>Vision</h4>
    <p>Image analysis and OCR via LLaVA multimodal models through Ollama.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-phase2">Experimental</span>
    <h4>LoRA Adapters</h4>
    <p>Load, unload, and manage GGUF LoRA adapters on top of base models.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-gpu">GPU</span>
    <h4>Music Generation</h4>
    <p>MusicGen text-to-music. Requires GPU with 4GB+ VRAM.</p>
  </div>
  <div class="feature-card">
    <span class="badge badge-ready">Ready</span>
    <h4>Browser Desktop</h4>
    <p>Full desktop environment accessible via noVNC at port 6080.</p>
  </div>
</div>

<!-- CLI -->
<h2>CLI Quick Reference</h2>
<pre><code>lorai start [--gpu] [--port N]   <span class="cmt"># Start the LorAI container</span>
lorai stop                       <span class="cmt"># Stop the container</span>
lorai status                     <span class="cmt"># Show system status</span>
lorai chat <span class="str">"What is LorAI?"</span>       <span class="cmt"># Chat with the local AI</span>
lorai desktop                    <span class="cmt"># Open browser desktop (noVNC)</span>
lorai pull llama3                <span class="cmt"># Download an AI model</span>
lorai bench                      <span class="cmt"># Benchmark your hardware</span>
lorai logs                       <span class="cmt"># Show container logs</span>
lorai version                    <span class="cmt"># Show version info</span></code></pre>

<!-- Architecture -->
<h2>Architecture</h2>
<pre><code>+-----------------------------------------------------------+
| Your code (Python, Node.js, Go, Rust, Java, C#, Ruby...) |
|   from lorai import LorAI    // or any OpenAI SDK         |
|   ai = LorAI()               // or new OpenAI({baseURL})  |
+---------------------------+-------------------------------+
                            |
                            | HTTP (OpenAI-compatible + LorAI-native)
                            v
+-----------------------------------------------------------+
| Docker: getlorai/desktop                                  |
| +-------------------------------------------------------+ |
| | FastAPI Gateway (port 1842)                           | |
| |   /v1/*    -> Ollama proxy (OpenAI-compatible)        | |
| |   /lorai/* -> Native services (RAG, agents, etc.)     | |
| +------------------------+------------------------------+ |
|                          v                                |
| +----------+ +----------+ +----------+ +--------------+   |
| |  Ollama  | | ChromaDB | | Whisper  | |    noVNC     |   |
| | :11434   | | (vector) | | + Piper  | |    :6080     |   |
| +----------+ +----------+ +----------+ +--------------+   |
+-----------------------------------------------------------+</code></pre>

<table>
  <tr><th>Decision</th><th>Choice</th><th>Why</th></tr>
  <tr><td>Port</td><td><code>1842</code></td><td>Ada Lovelace's year &mdash; unique and memorable</td></tr>
  <tr><td>LLM server</td><td>Ollama</td><td>Best DX, OpenAI-compatible out of the box</td></tr>
  <tr><td>Default model</td><td>phi3:mini</td><td>Runs on CPU, small, fast, good quality</td></tr>
  <tr><td>API gateway</td><td>FastAPI</td><td>Async, auto-docs, Python ecosystem</td></tr>
  <tr><td>SDK base</td><td>OpenAI Python SDK</td><td>Zero learning curve</td></tr>
  <tr><td>Vector DB</td><td>ChromaDB</td><td>Embedded, zero config, persistent</td></tr>
  <tr><td>Desktop</td><td>Openbox + noVNC</td><td>Lightweight, browser-accessible</td></tr>
  <tr><td>Base image</td><td>Alpine 3.19</td><td>~5 MB base, minimal attack surface</td></tr>
</table>

<!-- Beta Notice -->
<h2>Beta Release</h2>
<div class="callout callout-info">
  <strong>LorAI v0.1.0-beta</strong> &mdash; This is the first public Beta release. The core API is stable,
  but some features are still being refined. We're looking for early adopters to test the platform and provide feedback.
  <br><br>
  <a href="changelog.html">View full changelog &rarr;</a>
</div>

<h3>What's included in Beta</h3>
<ul>
  <li><strong>27 API endpoints</strong> &mdash; 8 OpenAI-compatible + 19 LorAI-native</li>
  <li><strong>Python SDK</strong> with Docker auto-management, 9 service wrappers, and CLI</li>
  <li><strong>10 language integrations</strong> &mdash; Python, Node.js, Go, Rust, Java, Kotlin, C#, Ruby, PHP, cURL</li>
  <li><strong>13 AI capabilities</strong> &mdash; Chat, embeddings, image gen, video, voice, RAG, agents, code execution, vision, LoRA, music, model hub, browser desktop</li>
  <li><strong>CPU + GPU support</strong> &mdash; Core features work on CPU; image/video/music gen require NVIDIA GPU</li>
</ul>

<h3>Known Limitations</h3>
<ul>
  <li>First startup pulls ~2 GB Docker image + ~2.3 GB default model &mdash; allow several minutes</li>
  <li>No authentication &mdash; designed for local development, not internet-facing deployment</li>
  <li>GPU features require NVIDIA GPU + CUDA + nvidia-container-toolkit</li>
  <li>Windows support requires WSL2 with Docker Desktop</li>
  <li>LoRA adapter loading is experimental</li>
</ul>

</main>

<footer>
  LorAI v0.1.0-beta &mdash; MIT License &mdash; <a href="https://github.com/GajapathiKS/lorai-workspace">GitHub</a>
  &mdash; <a href="changelog.html">Changelog</a>
</footer>

</body>
</html>
