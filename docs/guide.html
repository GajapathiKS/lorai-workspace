<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Getting Started — LorAI Workspace</title>
  <meta name="description" content="Get started with LorAI Workspace Workspace in minutes. Install with pip, chat with local LLMs, generate images, use RAG, run agents — all from Python or CLI.">
  <meta name="keywords" content="LorAI Workspace guide, getting started, local AI setup, pip install lorai-workspace, AI tutorial, Docker AI">
  <meta name="author" content="LorAI Workspace Team">
  <meta name="robots" content="index, follow">
  <link rel="canonical" href="https://gajapathiks.github.io/lorai-workspace/guide.html">

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://gajapathiks.github.io/lorai-workspace/guide.html">
  <meta property="og:title" content="Getting Started — LorAI Workspace">
  <meta property="og:description" content="Get started with LorAI Workspace Workspace in minutes. Install with pip, chat with local LLMs, generate images, use RAG, run agents — all from Python or CLI.">
  <meta property="og:image" content="https://gajapathiks.github.io/lorai-workspace/og-image.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:site_name" content="LorAI Workspace">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Getting Started — LorAI Workspace">
  <meta name="twitter:description" content="Get started with LorAI Workspace Workspace in minutes. Install with pip, chat with local LLMs, generate images, use RAG, run agents — all from Python or CLI.">
  <meta name="twitter:image" content="https://gajapathiks.github.io/lorai-workspace/og-image.png">

  <!-- Theme -->
  <meta name="theme-color" content="#0d1117">
  <meta name="color-scheme" content="dark">

  <link rel="stylesheet" href="style.css">
</head>
<body>

<nav>
  <div class="nav-inner">
    <a href="index.html" class="logo">Lor<span>AI</span> Workspace</a>
    <div class="nav-links">
      <a href="index.html">Home</a>
      <a href="guide.html" class="active">Guide</a>
      <a href="sdk.html">SDK</a>
      <a href="api.html">API</a>
      <a href="integrations.html">Other Languages</a>
      <a href="https://github.com/GajapathiKS/lorai-workspace">GitHub</a>
    </div>
  </div>
</nav>

<main>
<div class="docs-layout">

<aside class="sidebar">
  <ul>
    <li class="section-label">Getting Started</li>
    <li><a href="#prerequisites">Prerequisites</a></li>
    <li><a href="#install">Installation</a></li>
    <li><a href="#quickstart">Quickstart</a></li>
    <li><a href="#cli">CLI Usage</a></li>
    <li class="section-label">Common Tasks</li>
    <li><a href="#chat">Chat with LLMs</a></li>
    <li><a href="#models">Managing Models</a></li>
    <li><a href="#images">Image Generation</a></li>
    <li><a href="#voice">Voice (STT/TTS)</a></li>
    <li><a href="#rag">RAG / Knowledge</a></li>
    <li><a href="#agents">Agents</a></li>
    <li><a href="#code">Code Execution</a></li>
    <li><a href="#vision">Vision &amp; OCR</a></li>
    <li><a href="#lora">LoRA Adapters</a></li>
    <li class="section-label">Reference</li>
    <li><a href="#gpu">GPU Setup</a></li>
    <li><a href="#ports">Ports &amp; URLs</a></li>
    <li><a href="#data">Data Persistence</a></li>
  </ul>
</aside>

<div>

<h2>Getting Started</h2>

<p>
  LorAI Workspace is a <strong>local AI platform</strong> that runs as a Docker container on your machine.
  The <code>LorAI</code> Python client gives you chat, image generation, voice, document search, agents, and more &mdash; all free, private, and offline.
  You never need to touch Docker directly; the client handles the container for you.
</p>

<!-- Prerequisites -->
<h3 id="prerequisites">Prerequisites</h3>
<ul>
  <li><strong>Python 3.9+</strong></li>
  <li><strong>~5 GB free disk space</strong> &mdash; for the Docker image (~2 GB) and default model (~2.3 GB)</li>
  <li><strong>GPU (optional)</strong> &mdash; only needed for image/video/music generation. LLMs work fine on CPU.</li>
</ul>

<!-- Step 1: Install Docker -->
<h3 id="install-docker">Step 1 &mdash; Install Docker</h3>
<p>LorAI Workspace runs inside a Docker container, so Docker must be installed first. Pick your OS:</p>
<ul>
  <li><strong>macOS</strong> &mdash; <a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank">Download Docker Desktop for Mac →</a></li>
  <li><strong>Windows</strong> &mdash; <a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank">Download Docker Desktop for Windows →</a></li>
  <li><strong>Linux</strong> &mdash; run the official install script:
    <pre><code>curl -fsSL https://get.docker.com | sh
sudo systemctl enable --now docker
sudo usermod -aG docker $USER  # log out and back in after this</code></pre>
  </li>
</ul>
<p>Verify Docker is working before continuing:</p>
<pre><code>docker run hello-world</code></pre>

<!-- Step 2: Install SDK -->
<h3 id="install">Step 2 &mdash; Install the SDK</h3>
<pre><code>pip install lorai-workspace</code></pre>
<p>That's it. The Docker image (~2 GB) and default model (~2.3 GB) pull automatically on first use — you don't need to run any <code>docker</code> commands manually.</p>

<!-- Quickstart -->
<h3 id="quickstart">Quickstart</h3>

<div class="editor">
  <div class="editor-tabs">
    <span class="editor-tab">quickstart.py</span>
    <span class="editor-lang">Python</span>
  </div>
  <pre><code><span class="kw">from</span> lorai_workspace <span class="kw">import</span> LorAI

<span class="cmt"># Pulls Docker image (~2 GB) and starts the container on first run</span>
<span class="cmt"># Subsequent runs skip this — container is already up</span>
ai = <span class="fn">LorAI</span>()

<span class="fn">print</span>(ai.<span class="fn">chat</span>(<span class="str">"What is 12 multiplied by 8?"</span>))
<span class="cmt"># → "12 multiplied by 8 is 96."</span>

<span class="cmt"># Common options</span>
ai = <span class="fn">LorAI</span>(auto_start=<span class="kw">False</span>)   <span class="cmt"># container already running</span>
ai.<span class="fn">chat</span>(<span class="str">"Hello"</span>, model=<span class="str">"llama3"</span>)  <span class="cmt"># use a specific model</span>
ai.<span class="fn">stop</span>()                          <span class="cmt"># stop when done</span></code></pre>
</div>

<!-- CLI -->
<h3 id="cli">CLI Usage</h3>
<p>The <code>lorai</code> command is installed with the SDK. All commands:</p>
<pre><code><span class="cmt"># Start / Stop</span>
lorai-workspace start                      <span class="cmt"># start the container</span>
lorai-workspace start --gpu                <span class="cmt"># start with GPU passthrough</span>
lorai-workspace start --port 8080          <span class="cmt"># use a custom port</span>
lorai-workspace stop                       <span class="cmt"># stop and remove the container</span>

<span class="cmt"># Interact</span>
lorai-workspace chat <span class="str">"What is quantum computing?"</span>
lorai-workspace chat <span class="str">"Translate to French: hello"</span> --model llama3

<span class="cmt"># Models</span>
lorai-workspace pull llama3                <span class="cmt"># download a model</span>
lorai-workspace status                     <span class="cmt"># show system status</span>
lorai-workspace bench                      <span class="cmt"># benchmark inference speed</span>

<span class="cmt"># Desktop &amp; Logs</span>
lorai-workspace desktop                    <span class="cmt"># open noVNC desktop in browser</span>
lorai-workspace logs                       <span class="cmt"># tail container logs</span>
lorai-workspace version                    <span class="cmt"># show version info</span></code></pre>

<!-- Chat with LLMs -->
<h3 id="chat">Chat with LLMs</h3>
<p>Use this for any text task: Q&amp;A, summarisation, code generation, translation, classification, and more. The default model is <strong>phi3:mini</strong>, which runs on CPU and handles most tasks well.</p>

<h4>Simple chat</h4>
<pre><code>ai = LorAI(auto_start=<span class="kw">False</span>)

answer = ai.chat(<span class="str">"What is the capital of France?"</span>)
<span class="fn">print</span>(answer)
<span class="cmt"># → "The capital of France is Paris."</span>

<span class="cmt"># Ask it to summarise or rewrite text</span>
summary = ai.chat(<span class="str">"Summarise this in one sentence: "</span> + long_text)

<span class="cmt"># Translate</span>
translated = ai.chat(<span class="str">"Translate to Spanish: Good morning, how are you?"</span>)
<span class="cmt"># → "Buenos días, ¿cómo estás?"</span></code></pre>

<h4>With a system prompt</h4>
<p>A system prompt shapes the model's behaviour for the whole conversation. Use it to set a persona, enforce a format, or add context.</p>
<pre><code>answer = ai.chat(
    <span class="str">"Explain recursion"</span>,
    system=<span class="str">"You are a CS professor teaching first-year students. Use simple analogies."</span>,
    temperature=<span class="num">0.5</span>,   <span class="cmt"># lower = more factual, higher = more creative</span>
)</code></pre>

<h4>JSON mode</h4>
<p>Useful when you need structured output to feed into other code.</p>
<pre><code>data = ai.chat(
    <span class="str">"List 3 planets as JSON. Fields: name, diameter_km, has_rings."</span>,
    json_mode=<span class="kw">True</span>,
)
<span class="cmt"># → '{"planets": [{"name": "Earth", "diameter_km": 12742, "has_rings": false}, ...]}'</span>
<span class="kw">import</span> json
planets = json.loads(data)</code></pre>

<h4>Streaming</h4>
<p>Stream tokens as they are generated — useful for long responses or interactive UIs.</p>
<pre><code>ai.chat(<span class="str">"Write a short story about a robot"</span>, stream=<span class="kw">True</span>)
<span class="cmt"># tokens print to stdout as they arrive</span></code></pre>


<!-- Managing Models -->
<h3 id="models">Managing Models</h3>
<p>LorAI Workspace uses <a href="https://ollama.com">Ollama</a> under the hood to manage LLMs. You can pull any model from the Ollama library. Larger models produce better output but require more RAM and run slower.</p>

<table>
  <tr><th>Model</th><th>Size</th><th>Best for</th></tr>
  <tr><td><code>phi3:mini</code> (default)</td><td>2.3 GB</td><td>Fast, general use, works on CPU</td></tr>
  <tr><td><code>llama3</code></td><td>4.7 GB</td><td>Higher quality answers, needs 8 GB RAM</td></tr>
  <tr><td><code>llama3.2</code></td><td>2.0 GB</td><td>Good balance of speed and quality</td></tr>
  <tr><td><code>codellama</code></td><td>3.8 GB</td><td>Code generation and review</td></tr>
  <tr><td><code>mistral</code></td><td>4.1 GB</td><td>Strong reasoning and instruction following</td></tr>
</table>

<pre><code><span class="cmt"># Download a model (saved to ~/.lorai/data/models/)</span>
ai.hub.pull(<span class="str">"llama3"</span>)

<span class="cmt"># See what's downloaded</span>
models = ai.hub.models()

<span class="cmt"># Use a specific model for a chat</span>
answer = ai.chat(<span class="str">"Explain black holes"</span>, model=<span class="str">"llama3"</span>)

<span class="cmt"># Free up disk space</span>
ai.hub.remove(<span class="str">"phi3:mini"</span>)

<span class="cmt"># Check CPU, RAM, GPU and which models are loaded</span>
status = ai.hub.status()

<span class="cmt"># Measure how fast your hardware runs inference (tokens/sec)</span>
result = ai.hub.bench()</code></pre>

<p>You can also manage models from the CLI:</p>
<pre><code>lorai-workspace pull llama3        <span class="cmt"># download</span>
lorai-workspace status             <span class="cmt"># see loaded models + system resources</span>
lorai-workspace bench              <span class="cmt"># benchmark</span></code></pre>

<!-- Image Generation -->
<h3 id="images">Image Generation</h3>
<p>Generate images from a text description, or modify an existing image. Uses SDXL Turbo. <strong>Requires an NVIDIA GPU with 8 GB+ VRAM</strong> — this won't work on CPU.</p>
<pre><code><span class="cmt"># Start LorAI Workspace with GPU passthrough enabled</span>
ai = LorAI(gpu=<span class="kw">True</span>)

<span class="cmt"># Generate an image and save it to disk</span>
ai.image.generate(
    <span class="str">"A cozy coffee shop in the rain, impressionist painting style"</span>,
    size=<span class="str">"1024x1024"</span>,
    save_to=<span class="str">"coffee_shop.png"</span>,
)

<span class="cmt"># Edit an existing photo</span>
ai.image.edit(<span class="str">"portrait.jpg"</span>, <span class="str">"Add a futuristic cyberpunk background"</span>)</code></pre>

<!-- Voice -->
<h3 id="voice">Voice (STT + TTS)</h3>
<p>Convert audio to text (speech-to-text) using Whisper, or generate spoken audio from text (text-to-speech) using Piper. Both run entirely locally.</p>
<pre><code><span class="cmt"># Transcribe a recording to text (supports mp3, wav, m4a, ogg)</span>
text = ai.voice.transcribe(<span class="str">"meeting_notes.mp3"</span>)
<span class="fn">print</span>(text)
<span class="cmt"># → "Good morning everyone, let's get started with the weekly sync..."</span>

<span class="cmt"># Convert text to spoken audio and save as .wav</span>
ai.voice.speak(
    <span class="str">"Your report has been generated and is ready for download."</span>,
    voice=<span class="str">"nova"</span>,           <span class="cmt"># options: nova, alloy, echo, fable, onyx, shimmer</span>
    save_to=<span class="str">"notification.wav"</span>,
)</code></pre>

<!-- RAG / Knowledge -->
<h3 id="rag">RAG / Knowledge Base</h3>
<p>RAG (Retrieval-Augmented Generation) lets you point the LLM at your own documents so it can answer questions about them. Useful for internal documentation, codebases, PDFs, and research notes. Documents are stored in ChromaDB and persist across restarts.</p>
<pre><code><span class="cmt"># Step 1: Ingest your documents (do this once, or re-run when docs change)</span>
ai.knowledge.ingest(<span class="str">"/path/to/your/docs/"</span>, collection=<span class="str">"my-project"</span>)
<span class="cmt"># Recursively reads .txt, .md, .pdf files and indexes them</span>

<span class="cmt"># Step 2: Ask questions — the model grounds its answer in your docs</span>
answer = ai.knowledge.ask(<span class="str">"What is the process for requesting time off?"</span>)
<span class="fn">print</span>(answer)
<span class="cmt"># → "According to the employee handbook, time off requests should be..."</span>

<span class="cmt"># Or do a raw semantic search to find the relevant passages yourself</span>
results = ai.knowledge.search(<span class="str">"authentication and authorisation"</span>, top_k=<span class="num">5</span>)
<span class="kw">for</span> r <span class="kw">in</span> results:
    <span class="fn">print</span>(r[<span class="str">"text"</span>], r[<span class="str">"source"</span>])</code></pre>

<!-- Agents -->
<h3 id="agents">Agents</h3>
<p>Agents can plan and execute multi-step tasks using tools like file read/write, web search, code execution, and knowledge lookup. Give them a goal in plain English and they figure out the steps.</p>
<pre><code><span class="cmt"># Ask the agent to complete a task autonomously</span>
result = ai.agents.run(
    <span class="str">"Read all .py files in ./src/, find the three largest functions, and write a summary to summary.md"</span>,
    max_steps=<span class="num">15</span>,
)
<span class="fn">print</span>(result[<span class="str">"output"</span>])

<span class="cmt"># See which agents and tools are available</span>
<span class="fn">print</span>(ai.agents.list_agents())
<span class="fn">print</span>(ai.agents.list_tools())</code></pre>

<!-- Code Execution -->
<h3 id="code">Code Generation &amp; Review</h3>
<p>Ask the model to write code and optionally run it inside the container's sandboxed environment. Results come back as text.</p>
<pre><code><span class="cmt"># Generate code and run it immediately</span>
result = ai.code.generate(
    <span class="str">"Write a Python function that checks if a string is a palindrome, then test it"</span>,
    language=<span class="str">"python"</span>,
    execute=<span class="kw">True</span>,
)
<span class="fn">print</span>(result[<span class="str">"code"</span>])    <span class="cmt"># the generated code</span>
<span class="fn">print</span>(result[<span class="str">"output"</span>])  <span class="cmt"># stdout from running it</span>

<span class="cmt"># Have the model review existing code and suggest improvements</span>
feedback = ai.code.review(<span class="str">"""
def get_user(id):
    conn = connect(DB_URL)
    return conn.execute(f"SELECT * FROM users WHERE id = {id}").fetchone()
"""</span>)
<span class="cmt"># → "This function has a SQL injection vulnerability. Use parameterised queries..."</span></code></pre>

<!-- Vision -->
<h3 id="vision">Vision &amp; OCR</h3>
<p>Analyse images using LLaVA, a multimodal model that understands images and text together. Extract text from screenshots with OCR.</p>
<pre><code><span class="cmt"># Describe or answer questions about an image</span>
description = ai.vision.analyze(<span class="str">"chart.png"</span>, <span class="str">"What trend does this graph show?"</span>)
<span class="fn">print</span>(description)
<span class="cmt"># → "The graph shows a steady upward trend in monthly revenue from Jan to June..."</span>

<span class="cmt"># Extract text from a screenshot or photo of a document</span>
text = ai.vision.ocr(<span class="str">"receipt.jpg"</span>)
<span class="fn">print</span>(text)
<span class="cmt"># → "Café Nero\nDate: 14 Feb 2026\nFlat White x1 ... £3.50\nTotal: £3.50"</span></code></pre>

<!-- LoRA -->
<h3 id="lora">LoRA Adapters</h3>
<p>LoRA adapters fine-tune a base model's style or domain knowledge without retraining it from scratch. Load a <code>.gguf</code> adapter file and use it alongside any Ollama base model. This feature is experimental.</p>
<pre><code><span class="cmt"># Put your .gguf adapter in ~/.lorai/data/loras/ first</span>

<span class="cmt"># See what adapters are available</span>
loras = ai.lora.list()

<span class="cmt"># Load an adapter on top of a base model</span>
ai.lora.load(<span class="str">"my-finetune"</span>, base_model=<span class="str">"llama3.2"</span>)

<span class="cmt"># Chat using the fine-tuned model</span>
answer = ai.chat(<span class="str">"Write a product description for noise-cancelling headphones"</span>, lora=<span class="str">"my-finetune"</span>)

<span class="cmt"># Unload when done to free memory</span>
ai.lora.unload(<span class="str">"llama3.2-my-finetune"</span>)</code></pre>

<!-- OpenAI SDK compatibility -->
<h3 id="openai-compat">OpenAI SDK Compatibility</h3>
<p>
  If you already have code written for the OpenAI Python SDK, it works with LorAI Workspace unchanged.
  The underlying API speaks the same protocol &mdash; just swap <code>OpenAI()</code> for <code>LorAI()</code>.
</p>
<pre><code><span class="cmt"># Existing OpenAI SDK code works as-is</span>
response = ai.chat.completions.create(
    model=<span class="str">"phi3:mini"</span>,
    messages=[
        {<span class="str">"role"</span>: <span class="str">"system"</span>, <span class="str">"content"</span>: <span class="str">"You are a helpful assistant."</span>},
        {<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"What's a good name for a Python package?"</span>},
    ],
    temperature=<span class="num">0.7</span>,
    max_tokens=<span class="num">200</span>,
)
<span class="fn">print</span>(response.choices[<span class="num">0</span>].message.content)

<span class="cmt"># Or point any HTTP client at localhost:1842/v1 directly</span>
<span class="cmt"># See the Integrations page for Node.js, Go, Rust, cURL, and more</span></code></pre>

<!-- GPU Setup -->
<h3 id="gpu">GPU Setup</h3>
<p>For GPU features (image gen, video, music), you need:</p>
<ol>
  <li>An NVIDIA GPU with CUDA drivers installed</li>
  <li>The <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">NVIDIA Container Toolkit</a></li>
</ol>
<p>Then start LorAI with GPU:</p>
<pre><code><span class="cmt"># Via CLI</span>
lorai-workspace start --gpu

<span class="cmt"># Via Python</span>
ai = LorAI(gpu=<span class="kw">True</span>)</code></pre>

<table>
  <tr><th>Feature</th><th>Min VRAM</th></tr>
  <tr><td>LLMs (Ollama)</td><td>CPU works, GPU faster</td></tr>
  <tr><td>Image generation (SDXL Turbo)</td><td>8 GB</td></tr>
  <tr><td>Video generation (CogVideoX)</td><td>12 GB</td></tr>
  <tr><td>Music generation (MusicGen)</td><td>4 GB</td></tr>
</table>

<!-- Ports & URLs -->
<h3 id="ports">Ports &amp; URLs</h3>
<table>
  <tr><th>Port</th><th>Service</th><th>URL</th></tr>
  <tr><td><code>1842</code></td><td>API Gateway</td><td><code>http://localhost:1842</code></td></tr>
  <tr><td><code>6080</code></td><td>noVNC Desktop</td><td><code>http://localhost:6080</code></td></tr>
  <tr><td><code>11434</code></td><td>Ollama (internal)</td><td>Not exposed externally</td></tr>
</table>

<p>Custom port:</p>
<pre><code>ai = LorAI(port=<span class="num">8080</span>)    <span class="cmt"># API on 8080 instead of 1842</span></code></pre>

<!-- Data Persistence -->
<h3 id="data">Data Persistence</h3>
<p>
  LorAI Workspace mounts <code>~/.lorai/data</code> on the host to <code>/data</code> inside the container.
  This persists across container restarts:
</p>
<ul>
  <li><code>/data/vectors/</code> &mdash; ChromaDB vector store</li>
  <li><code>/data/models/</code> &mdash; Cached models</li>
  <li><code>/data/loras/</code> &mdash; LoRA adapter files</li>
  <li><code>/data/config/</code> &mdash; User configuration</li>
</ul>

</div>
</div>
</main>

<footer>
  LorAI Workspace v0.1.0-beta &mdash; MIT License &mdash; <a href="https://github.com/GajapathiKS/lorai-workspace">GitHub</a>
</footer>

</body>
</html>
