<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Getting Started — LorAI</title>
  <meta name="description" content="Get started with LorAI in minutes. Install with pip, chat with local LLMs, generate images, use RAG, run agents — all from Python or CLI.">
  <meta name="keywords" content="LorAI guide, getting started, local AI setup, pip install lorai-workspace, AI tutorial, Docker AI">
  <meta name="author" content="LorAI Team">
  <meta name="robots" content="index, follow">
  <link rel="canonical" href="https://gajapathiks.github.io/lorai-workspace/guide.html">

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://gajapathiks.github.io/lorai-workspace/guide.html">
  <meta property="og:title" content="Getting Started — LorAI">
  <meta property="og:description" content="Get started with LorAI in minutes. Install with pip, chat with local LLMs, generate images, use RAG, run agents — all from Python or CLI.">
  <meta property="og:image" content="https://gajapathiks.github.io/lorai-workspace/og-image.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:site_name" content="LorAI">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Getting Started — LorAI">
  <meta name="twitter:description" content="Get started with LorAI in minutes. Install with pip, chat with local LLMs, generate images, use RAG, run agents — all from Python or CLI.">
  <meta name="twitter:image" content="https://gajapathiks.github.io/lorai-workspace/og-image.png">

  <!-- Theme -->
  <meta name="theme-color" content="#0d1117">
  <meta name="color-scheme" content="dark">

  <link rel="stylesheet" href="style.css">
</head>
<body>

<nav>
  <div class="nav-inner">
    <a href="index.html" class="logo">Lor<span>AI</span></a>
    <div class="nav-links">
      <a href="index.html">Home</a>
      <a href="guide.html" class="active">Guide</a>
      <a href="sdk.html">SDK</a>
      <a href="api.html">API</a>
      <a href="integrations.html">Integrations</a>
      <a href="https://github.com/GajapathiKS/lorai-workspace">GitHub</a>
    </div>
  </div>
</nav>

<main>
<div class="docs-layout">

<aside class="sidebar">
  <ul>
    <li class="section-label">Getting Started</li>
    <li><a href="#prerequisites">Prerequisites</a></li>
    <li><a href="#install">Installation</a></li>
    <li><a href="#quickstart">Quickstart</a></li>
    <li><a href="#cli">CLI Usage</a></li>
    <li class="section-label">Common Tasks</li>
    <li><a href="#chat">Chat with LLMs</a></li>
    <li><a href="#models">Managing Models</a></li>
    <li><a href="#images">Image Generation</a></li>
    <li><a href="#voice">Voice (STT/TTS)</a></li>
    <li><a href="#rag">RAG / Knowledge</a></li>
    <li><a href="#agents">Agents</a></li>
    <li><a href="#code">Code Execution</a></li>
    <li><a href="#vision">Vision &amp; OCR</a></li>
    <li><a href="#lora">LoRA Adapters</a></li>
    <li class="section-label">Reference</li>
    <li><a href="#gpu">GPU Setup</a></li>
    <li><a href="#ports">Ports &amp; URLs</a></li>
    <li><a href="#data">Data Persistence</a></li>
  </ul>
</aside>

<div>

<h2>Getting Started</h2>

<p>
  LorAI is a local AI platform that runs as a Docker container on your machine.
  It gives you an OpenAI-compatible API on <code>localhost:1842</code> for chat, image generation,
  voice, document search (RAG), agents, and more — all free, private, and offline.
  The Python SDK handles the Docker container for you, so you never need to touch Docker directly.
</p>

<!-- Prerequisites -->
<h3 id="prerequisites">Prerequisites</h3>
<ul>
  <li><strong>Python 3.9+</strong></li>
  <li><strong>Docker</strong> installed and running &mdash; <a href="https://docs.docker.com/get-docker/">install Docker</a> if you haven't already</li>
  <li><strong>~5 GB free disk space</strong> &mdash; for the Docker image (~2 GB) and default model (~2.3 GB)</li>
  <li><strong>GPU (optional)</strong> &mdash; only needed for image/video/music generation. LLMs work fine on CPU.</li>
</ul>

<!-- Installation -->
<h3 id="install">Installation</h3>
<pre><code>pip install lorai-workspace</code></pre>
<p>That's the only install step. The Docker image pulls automatically the first time you use LorAI.</p>

<!-- Quickstart -->
<h3 id="quickstart">Quickstart</h3>
<pre><code><span class="kw">from</span> lorai <span class="kw">import</span> LorAI

ai = LorAI()
<span class="fn">print</span>(ai.chat(<span class="str">"What is 12 multiplied by 8?"</span>))</code></pre>

<p>On <strong>first run</strong>, you'll see LorAI bootstrap itself in the terminal:</p>
<pre><code>  ██╗      ██████╗  ██████╗  █████╗ ██╗
  ...
  Pulling image gajapathiks/lorai-desktop:latest ... done
  Starting container lorai on port 1842 ...
  Waiting for Ollama ... ready
  Pulling phi3:mini (2.3 GB) ... done
  LorAI is ready at http://localhost:1842

96</code></pre>

<p><strong>On subsequent runs</strong>, the container is already up so it skips straight to your code.</p>

<p>Common options:</p>
<pre><code><span class="cmt"># Container already running (e.g. started with 'lorai start')</span>
ai = LorAI(auto_start=<span class="kw">False</span>)

<span class="cmt"># Use a specific model instead of the default phi3:mini</span>
response = ai.chat(<span class="str">"Hello"</span>, model=<span class="str">"llama3"</span>)

<span class="cmt"># Stop the container when you're done</span>
ai.stop()</code></pre>

<!-- CLI -->
<h3 id="cli">CLI Usage</h3>
<p>The <code>lorai</code> command is installed with the SDK. All commands:</p>
<pre><code><span class="cmt"># Start / Stop</span>
lorai start                      <span class="cmt"># start the container</span>
lorai start --gpu                <span class="cmt"># start with GPU passthrough</span>
lorai start --port 8080          <span class="cmt"># use a custom port</span>
lorai stop                       <span class="cmt"># stop and remove the container</span>

<span class="cmt"># Interact</span>
lorai chat <span class="str">"What is quantum computing?"</span>
lorai chat <span class="str">"Translate to French: hello"</span> --model llama3

<span class="cmt"># Models</span>
lorai pull llama3                <span class="cmt"># download a model</span>
lorai status                     <span class="cmt"># show system status</span>
lorai bench                      <span class="cmt"># benchmark inference speed</span>

<span class="cmt"># Desktop &amp; Logs</span>
lorai desktop                    <span class="cmt"># open noVNC desktop in browser</span>
lorai logs                       <span class="cmt"># tail container logs</span>
lorai version                    <span class="cmt"># show version info</span></code></pre>

<!-- Chat with LLMs -->
<h3 id="chat">Chat with LLMs</h3>
<p>Use this for any text task: Q&amp;A, summarisation, code generation, translation, classification, and more. The default model is <strong>phi3:mini</strong>, which runs on CPU and handles most tasks well.</p>

<h4>Simple chat</h4>
<pre><code>ai = LorAI(auto_start=<span class="kw">False</span>)

answer = ai.chat(<span class="str">"What is the capital of France?"</span>)
<span class="fn">print</span>(answer)
<span class="cmt"># → "The capital of France is Paris."</span>

<span class="cmt"># Ask it to summarise or rewrite text</span>
summary = ai.chat(<span class="str">"Summarise this in one sentence: "</span> + long_text)

<span class="cmt"># Translate</span>
translated = ai.chat(<span class="str">"Translate to Spanish: Good morning, how are you?"</span>)
<span class="cmt"># → "Buenos días, ¿cómo estás?"</span></code></pre>

<h4>With a system prompt</h4>
<p>A system prompt shapes the model's behaviour for the whole conversation. Use it to set a persona, enforce a format, or add context.</p>
<pre><code>answer = ai.chat(
    <span class="str">"Explain recursion"</span>,
    system=<span class="str">"You are a CS professor teaching first-year students. Use simple analogies."</span>,
    temperature=<span class="num">0.5</span>,   <span class="cmt"># lower = more factual, higher = more creative</span>
)</code></pre>

<h4>JSON mode</h4>
<p>Useful when you need structured output to feed into other code.</p>
<pre><code>data = ai.chat(
    <span class="str">"List 3 planets as JSON. Fields: name, diameter_km, has_rings."</span>,
    json_mode=<span class="kw">True</span>,
)
<span class="cmt"># → '{"planets": [{"name": "Earth", "diameter_km": 12742, "has_rings": false}, ...]}'</span>
<span class="kw">import</span> json
planets = json.loads(data)</code></pre>

<h4>Streaming</h4>
<p>Stream tokens as they are generated — useful for long responses or interactive UIs.</p>
<pre><code>ai.chat(<span class="str">"Write a short story about a robot"</span>, stream=<span class="kw">True</span>)
<span class="cmt"># tokens print to stdout as they arrive</span></code></pre>

<h4>Drop-in for OpenAI SDK</h4>
<p>Because <code>LorAI</code> extends the OpenAI client, any code written for OpenAI works without changes.</p>
<pre><code><span class="cmt"># Existing OpenAI code — just replace OpenAI() with LorAI()</span>
response = ai.chat.completions.create(
    model=<span class="str">"phi3:mini"</span>,   <span class="cmt"># any model you've pulled with 'lorai pull &lt;name&gt;'</span>
    messages=[
        {<span class="str">"role"</span>: <span class="str">"system"</span>, <span class="str">"content"</span>: <span class="str">"You are a helpful assistant."</span>},
        {<span class="str">"role"</span>: <span class="str">"user"</span>, <span class="str">"content"</span>: <span class="str">"What's a good name for a Python package?"</span>},
    ],
    temperature=<span class="num">0.7</span>,
    max_tokens=<span class="num">200</span>,
)
<span class="fn">print</span>(response.choices[<span class="num">0</span>].message.content)</code></pre>

<!-- Managing Models -->
<h3 id="models">Managing Models</h3>
<p>LorAI uses <a href="https://ollama.com">Ollama</a> under the hood to manage LLMs. You can pull any model from the Ollama library. Larger models produce better output but require more RAM and run slower.</p>

<table>
  <tr><th>Model</th><th>Size</th><th>Best for</th></tr>
  <tr><td><code>phi3:mini</code> (default)</td><td>2.3 GB</td><td>Fast, general use, works on CPU</td></tr>
  <tr><td><code>llama3</code></td><td>4.7 GB</td><td>Higher quality answers, needs 8 GB RAM</td></tr>
  <tr><td><code>llama3.2</code></td><td>2.0 GB</td><td>Good balance of speed and quality</td></tr>
  <tr><td><code>codellama</code></td><td>3.8 GB</td><td>Code generation and review</td></tr>
  <tr><td><code>mistral</code></td><td>4.1 GB</td><td>Strong reasoning and instruction following</td></tr>
</table>

<pre><code><span class="cmt"># Download a model (saved to ~/.lorai/data/models/)</span>
ai.hub.pull(<span class="str">"llama3"</span>)

<span class="cmt"># See what's downloaded</span>
models = ai.hub.models()

<span class="cmt"># Use a specific model for a chat</span>
answer = ai.chat(<span class="str">"Explain black holes"</span>, model=<span class="str">"llama3"</span>)

<span class="cmt"># Free up disk space</span>
ai.hub.remove(<span class="str">"phi3:mini"</span>)

<span class="cmt"># Check CPU, RAM, GPU and which models are loaded</span>
status = ai.hub.status()

<span class="cmt"># Measure how fast your hardware runs inference (tokens/sec)</span>
result = ai.hub.bench()</code></pre>

<p>You can also manage models from the CLI:</p>
<pre><code>lorai pull llama3        <span class="cmt"># download</span>
lorai status             <span class="cmt"># see loaded models + system resources</span>
lorai bench              <span class="cmt"># benchmark</span></code></pre>

<!-- Image Generation -->
<h3 id="images">Image Generation</h3>
<p>Generate images from a text description, or modify an existing image. Uses SDXL Turbo. <strong>Requires an NVIDIA GPU with 8 GB+ VRAM</strong> — this won't work on CPU.</p>
<pre><code><span class="cmt"># Start LorAI with GPU passthrough enabled</span>
ai = LorAI(gpu=<span class="kw">True</span>)

<span class="cmt"># Generate an image and save it to disk</span>
ai.image.generate(
    <span class="str">"A cozy coffee shop in the rain, impressionist painting style"</span>,
    size=<span class="str">"1024x1024"</span>,
    save_to=<span class="str">"coffee_shop.png"</span>,
)

<span class="cmt"># Edit an existing photo</span>
ai.image.edit(<span class="str">"portrait.jpg"</span>, <span class="str">"Add a futuristic cyberpunk background"</span>)</code></pre>

<!-- Voice -->
<h3 id="voice">Voice (STT + TTS)</h3>
<p>Convert audio to text (speech-to-text) using Whisper, or generate spoken audio from text (text-to-speech) using Piper. Both run entirely locally.</p>
<pre><code><span class="cmt"># Transcribe a recording to text (supports mp3, wav, m4a, ogg)</span>
text = ai.voice.transcribe(<span class="str">"meeting_notes.mp3"</span>)
<span class="fn">print</span>(text)
<span class="cmt"># → "Good morning everyone, let's get started with the weekly sync..."</span>

<span class="cmt"># Convert text to spoken audio and save as .wav</span>
ai.voice.speak(
    <span class="str">"Your report has been generated and is ready for download."</span>,
    voice=<span class="str">"nova"</span>,           <span class="cmt"># options: nova, alloy, echo, fable, onyx, shimmer</span>
    save_to=<span class="str">"notification.wav"</span>,
)</code></pre>

<!-- RAG / Knowledge -->
<h3 id="rag">RAG / Knowledge Base</h3>
<p>RAG (Retrieval-Augmented Generation) lets you point the LLM at your own documents so it can answer questions about them. Useful for internal documentation, codebases, PDFs, and research notes. Documents are stored in ChromaDB and persist across restarts.</p>
<pre><code><span class="cmt"># Step 1: Ingest your documents (do this once, or re-run when docs change)</span>
ai.knowledge.ingest(<span class="str">"/path/to/your/docs/"</span>, collection=<span class="str">"my-project"</span>)
<span class="cmt"># Recursively reads .txt, .md, .pdf files and indexes them</span>

<span class="cmt"># Step 2: Ask questions — the model grounds its answer in your docs</span>
answer = ai.knowledge.ask(<span class="str">"What is the process for requesting time off?"</span>)
<span class="fn">print</span>(answer)
<span class="cmt"># → "According to the employee handbook, time off requests should be..."</span>

<span class="cmt"># Or do a raw semantic search to find the relevant passages yourself</span>
results = ai.knowledge.search(<span class="str">"authentication and authorisation"</span>, top_k=<span class="num">5</span>)
<span class="kw">for</span> r <span class="kw">in</span> results:
    <span class="fn">print</span>(r[<span class="str">"text"</span>], r[<span class="str">"source"</span>])</code></pre>

<!-- Agents -->
<h3 id="agents">Agents</h3>
<p>Agents can plan and execute multi-step tasks using tools like file read/write, web search, code execution, and knowledge lookup. Give them a goal in plain English and they figure out the steps.</p>
<pre><code><span class="cmt"># Ask the agent to complete a task autonomously</span>
result = ai.agents.run(
    <span class="str">"Read all .py files in ./src/, find the three largest functions, and write a summary to summary.md"</span>,
    max_steps=<span class="num">15</span>,
)
<span class="fn">print</span>(result[<span class="str">"output"</span>])

<span class="cmt"># See which agents and tools are available</span>
<span class="fn">print</span>(ai.agents.list_agents())
<span class="fn">print</span>(ai.agents.list_tools())</code></pre>

<!-- Code Execution -->
<h3 id="code">Code Generation &amp; Review</h3>
<p>Ask the model to write code and optionally run it inside the container's sandboxed environment. Results come back as text.</p>
<pre><code><span class="cmt"># Generate code and run it immediately</span>
result = ai.code.generate(
    <span class="str">"Write a Python function that checks if a string is a palindrome, then test it"</span>,
    language=<span class="str">"python"</span>,
    execute=<span class="kw">True</span>,
)
<span class="fn">print</span>(result[<span class="str">"code"</span>])    <span class="cmt"># the generated code</span>
<span class="fn">print</span>(result[<span class="str">"output"</span>])  <span class="cmt"># stdout from running it</span>

<span class="cmt"># Have the model review existing code and suggest improvements</span>
feedback = ai.code.review(<span class="str">"""
def get_user(id):
    conn = connect(DB_URL)
    return conn.execute(f"SELECT * FROM users WHERE id = {id}").fetchone()
"""</span>)
<span class="cmt"># → "This function has a SQL injection vulnerability. Use parameterised queries..."</span></code></pre>

<!-- Vision -->
<h3 id="vision">Vision &amp; OCR</h3>
<p>Analyse images using LLaVA, a multimodal model that understands images and text together. Extract text from screenshots with OCR.</p>
<pre><code><span class="cmt"># Describe or answer questions about an image</span>
description = ai.vision.analyze(<span class="str">"chart.png"</span>, <span class="str">"What trend does this graph show?"</span>)
<span class="fn">print</span>(description)
<span class="cmt"># → "The graph shows a steady upward trend in monthly revenue from Jan to June..."</span>

<span class="cmt"># Extract text from a screenshot or photo of a document</span>
text = ai.vision.ocr(<span class="str">"receipt.jpg"</span>)
<span class="fn">print</span>(text)
<span class="cmt"># → "Café Nero\nDate: 14 Feb 2026\nFlat White x1 ... £3.50\nTotal: £3.50"</span></code></pre>

<!-- LoRA -->
<h3 id="lora">LoRA Adapters</h3>
<p>LoRA adapters fine-tune a base model's style or domain knowledge without retraining it from scratch. Load a <code>.gguf</code> adapter file and use it alongside any Ollama base model. This feature is experimental.</p>
<pre><code><span class="cmt"># Put your .gguf adapter in ~/.lorai/data/loras/ first</span>

<span class="cmt"># See what adapters are available</span>
loras = ai.lora.list()

<span class="cmt"># Load an adapter on top of a base model</span>
ai.lora.load(<span class="str">"my-finetune"</span>, base_model=<span class="str">"llama3.2"</span>)

<span class="cmt"># Chat using the fine-tuned model</span>
answer = ai.chat(<span class="str">"Write a product description for noise-cancelling headphones"</span>, lora=<span class="str">"my-finetune"</span>)

<span class="cmt"># Unload when done to free memory</span>
ai.lora.unload(<span class="str">"llama3.2-my-finetune"</span>)</code></pre>

<!-- GPU Setup -->
<h3 id="gpu">GPU Setup</h3>
<p>For GPU features (image gen, video, music), you need:</p>
<ol>
  <li>An NVIDIA GPU with CUDA drivers installed</li>
  <li>The <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">NVIDIA Container Toolkit</a></li>
</ol>
<p>Then start LorAI with GPU:</p>
<pre><code><span class="cmt"># Via CLI</span>
lorai start --gpu

<span class="cmt"># Via Python</span>
ai = LorAI(gpu=<span class="kw">True</span>)</code></pre>

<table>
  <tr><th>Feature</th><th>Min VRAM</th></tr>
  <tr><td>LLMs (Ollama)</td><td>CPU works, GPU faster</td></tr>
  <tr><td>Image generation (SDXL Turbo)</td><td>8 GB</td></tr>
  <tr><td>Video generation (CogVideoX)</td><td>12 GB</td></tr>
  <tr><td>Music generation (MusicGen)</td><td>4 GB</td></tr>
</table>

<!-- Ports & URLs -->
<h3 id="ports">Ports &amp; URLs</h3>
<table>
  <tr><th>Port</th><th>Service</th><th>URL</th></tr>
  <tr><td><code>1842</code></td><td>API Gateway</td><td><code>http://localhost:1842</code></td></tr>
  <tr><td><code>6080</code></td><td>noVNC Desktop</td><td><code>http://localhost:6080</code></td></tr>
  <tr><td><code>11434</code></td><td>Ollama (internal)</td><td>Not exposed externally</td></tr>
</table>

<p>Custom port:</p>
<pre><code>ai = LorAI(port=<span class="num">8080</span>)    <span class="cmt"># API on 8080 instead of 1842</span></code></pre>

<!-- Data Persistence -->
<h3 id="data">Data Persistence</h3>
<p>
  LorAI mounts <code>~/.lorai/data</code> on the host to <code>/data</code> inside the container.
  This persists across container restarts:
</p>
<ul>
  <li><code>/data/vectors/</code> &mdash; ChromaDB vector store</li>
  <li><code>/data/models/</code> &mdash; Cached models</li>
  <li><code>/data/loras/</code> &mdash; LoRA adapter files</li>
  <li><code>/data/config/</code> &mdash; User configuration</li>
</ul>

</div>
</div>
</main>

<footer>
  LorAI v0.1.0-beta &mdash; MIT License &mdash; <a href="https://github.com/GajapathiKS/lorai-workspace">GitHub</a>
</footer>

</body>
</html>
